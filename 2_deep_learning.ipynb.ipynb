{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Deep Learning"]},{"cell_type":"markdown","metadata":{},"source":["## Import Module"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T07:46:19.129234Z","iopub.status.busy":"2023-11-23T07:46:19.128941Z","iopub.status.idle":"2023-11-23T07:46:31.767365Z","shell.execute_reply":"2023-11-23T07:46:31.766177Z","shell.execute_reply.started":"2023-11-23T07:46:19.129207Z"},"trusted":true},"outputs":[],"source":["!pip install captum"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-11-23T07:46:31.770216Z","iopub.status.busy":"2023-11-23T07:46:31.769828Z","iopub.status.idle":"2023-11-23T07:46:35.935939Z","shell.execute_reply":"2023-11-23T07:46:35.935151Z","shell.execute_reply.started":"2023-11-23T07:46:31.770174Z"},"trusted":true},"outputs":[],"source":["import os\n","import numpy as np\n","import random\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision.transforms import transforms\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import recall_score, f1_score, classification_report, accuracy_score, confusion_matrix\n","from captum.attr import IntegratedGradients"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T07:46:35.937527Z","iopub.status.busy":"2023-11-23T07:46:35.937016Z","iopub.status.idle":"2023-11-23T07:46:35.944418Z","shell.execute_reply":"2023-11-23T07:46:35.943403Z","shell.execute_reply.started":"2023-11-23T07:46:35.937497Z"},"trusted":true},"outputs":[],"source":["device = torch.device('cuda:0')\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T07:46:35.946755Z","iopub.status.busy":"2023-11-23T07:46:35.946483Z","iopub.status.idle":"2023-11-23T07:46:35.959499Z","shell.execute_reply":"2023-11-23T07:46:35.958676Z","shell.execute_reply.started":"2023-11-23T07:46:35.946730Z"},"trusted":true},"outputs":[],"source":["# 固定Seed\n","seed = 695\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","np.random.seed(seed)\n","random.seed(seed)\n","torch.backends.cudnn.benchmark = False\n","torch.backends.cudnn.deterministic = True"]},{"cell_type":"markdown","metadata":{},"source":["## Preprocessing\n"]},{"cell_type":"markdown","metadata":{},"source":["### Load data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T07:46:35.960744Z","iopub.status.busy":"2023-11-23T07:46:35.960457Z","iopub.status.idle":"2023-11-23T07:46:35.980769Z","shell.execute_reply":"2023-11-23T07:46:35.980040Z","shell.execute_reply.started":"2023-11-23T07:46:35.960719Z"},"trusted":true},"outputs":[],"source":["# control data\n","control_data_list = []\n","control_name_list = []\n","control_label_list = []\n","for dirname, _, filenames in os.walk('/kaggle/input/posner-data/control'):\n","    for file in filenames:\n","        control_data_list.append(os.path.join(dirname, file))\n","        control_name_list.append(file.split('_')[0])\n","        control_label_list.append('control')\n","control_data_list[:10], control_name_list[:10], control_label_list[:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T07:46:35.981968Z","iopub.status.busy":"2023-11-23T07:46:35.981738Z","iopub.status.idle":"2023-11-23T07:46:35.995767Z","shell.execute_reply":"2023-11-23T07:46:35.994915Z","shell.execute_reply.started":"2023-11-23T07:46:35.981947Z"},"trusted":true},"outputs":[],"source":["# ADHD data\n","ADHD_data_list = []\n","ADHD_name_list = []\n","ADHD_label_list = []\n","for dirname, _, filenames in os.walk('/kaggle/input/posner-data/ADHD'):\n","    for file in filenames:\n","        ADHD_data_list.append(os.path.join(dirname, file))\n","        ADHD_name_list.append(file.split('_')[0])\n","        ADHD_label_list.append('ADHD')\n","ADHD_data_list[:10], ADHD_name_list[:10], ADHD_label_list[:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T07:46:35.997003Z","iopub.status.busy":"2023-11-23T07:46:35.996776Z","iopub.status.idle":"2023-11-23T07:46:36.009246Z","shell.execute_reply":"2023-11-23T07:46:36.008377Z","shell.execute_reply.started":"2023-11-23T07:46:35.996982Z"},"trusted":true},"outputs":[],"source":["# TS data\n","TS_data_list = []\n","TS_name_list = []\n","TS_label_list = []\n","for dirname, _, filenames in os.walk('/kaggle/input/posner-data/TS'):\n","    for file in filenames:\n","        TS_data_list.append(os.path.join(dirname, file))\n","        TS_name_list.append(file.split('_')[0])\n","        TS_label_list.append('TS')\n","TS_data_list[:10], TS_name_list[:10], TS_label_list[:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T07:46:36.010551Z","iopub.status.busy":"2023-11-23T07:46:36.010290Z","iopub.status.idle":"2023-11-23T07:46:36.038060Z","shell.execute_reply":"2023-11-23T07:46:36.037201Z","shell.execute_reply.started":"2023-11-23T07:46:36.010527Z"},"trusted":true},"outputs":[],"source":["all_data_list = control_data_list + ADHD_data_list + TS_data_list\n","all_name_list = control_name_list + ADHD_name_list + TS_name_list\n","all_label_list = control_label_list + ADHD_label_list + TS_label_list\n","all_df = pd.DataFrame()\n","all_df[['data', 'name', 'label']] = pd.DataFrame(list(zip(all_data_list, all_name_list, all_label_list)))\n","all_df"]},{"cell_type":"markdown","metadata":{},"source":["### Padding"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T07:46:36.039387Z","iopub.status.busy":"2023-11-23T07:46:36.039097Z","iopub.status.idle":"2023-11-23T07:47:04.326283Z","shell.execute_reply":"2023-11-23T07:47:04.325295Z","shell.execute_reply.started":"2023-11-23T07:46:36.039341Z"},"trusted":true},"outputs":[],"source":["max_len = 0\n","for file in all_df['data']:\n","    data = pd.read_csv(file)\n","    if len(data) > max_len: \n","        max_len = len(data)\n","print(max_len)\n","blank = pd.Series(range(max_len))\n","for file in all_df['data']:\n","    data = pd.read_csv(file, usecols=[i for i in range(3, 7)])\n","    data_pad = pd.concat([blank, data], axis=1)\n","    data_pad = data_pad.fillna(0).drop(columns=[0])\n","    path = file.replace('input', 'working')\n","    if not os.path.isdir(os.path.split(path)[0]):\n","        os.makedirs(os.path.split(path)[0])\n","    \n","    avg = []\n","    for i in range(int(max_len/17)):\n","        avg.append(data_pad.iloc[i*17:(i+1)*17].mean(axis=0).values)\n","    \n","    data_final = pd.DataFrame(avg)\n","    data_final.T.to_csv(path, header=False, index=False)\n","    \n","all_df['data'] = all_df['data'].apply(lambda x: x.replace('input', 'working'))\n","all_df['label'] = all_df['label'].map({'control':0, 'ADHD':1, 'TS':2})"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T07:47:04.331500Z","iopub.status.busy":"2023-11-23T07:47:04.331185Z","iopub.status.idle":"2023-11-23T07:47:04.399576Z","shell.execute_reply":"2023-11-23T07:47:04.398530Z","shell.execute_reply.started":"2023-11-23T07:47:04.331472Z"},"trusted":true},"outputs":[],"source":["pd.read_csv(all_df['data'][0], header=None)"]},{"cell_type":"markdown","metadata":{},"source":["# Define Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T07:47:04.400924Z","iopub.status.busy":"2023-11-23T07:47:04.400642Z","iopub.status.idle":"2023-11-23T07:47:04.407820Z","shell.execute_reply":"2023-11-23T07:47:04.406807Z","shell.execute_reply.started":"2023-11-23T07:47:04.400898Z"},"trusted":true},"outputs":[],"source":["class CustomImageDataset(Dataset):\n","    def __init__(self, annotations_file, transform=None, target_transform=None):\n","        self.df = annotations_file\n","        self.transform = transform\n","        self.target_transform = target_transform\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.df.iloc[idx, 0]\n","        image = pd.read_csv(img_path, header=None).values\n","        name = self.df.iloc[idx, 1]\n","        label = self.df.iloc[idx, 2]\n","        if self.transform:\n","            image = self.transform(image)\n","        if self.target_transform:\n","            label = self.target_transform(label)\n","        return image, name, label"]},{"cell_type":"markdown","metadata":{},"source":["# Define net"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T07:47:04.409690Z","iopub.status.busy":"2023-11-23T07:47:04.409310Z","iopub.status.idle":"2023-11-23T07:47:04.426918Z","shell.execute_reply":"2023-11-23T07:47:04.426016Z","shell.execute_reply.started":"2023-11-23T07:47:04.409656Z"},"trusted":true},"outputs":[],"source":["class Network(nn.Module):\n","    def __init__(self, pool = 6, fc1=1024, maxpool=2, conv8_open=False):\n","        super(Network, self).__init__()\n","        \n","        self.maxpool = maxpool\n","        \n","        self.conv1 = nn.Conv1d(in_channels=4, out_channels=16, kernel_size=10, stride=1, padding=1)\n","        self.bn1 = nn.BatchNorm1d(16)        \n","        \n","        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=10, stride=1, padding=1)\n","        self.bn2 = nn.BatchNorm1d(32)\n","        self.pool = nn.MaxPool1d(self.maxpool, padding= 1)\n","        \n","        self.conv4 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=10, stride=1, padding=1)\n","        self.bn4 = nn.BatchNorm1d(32)\n","        self.conv5 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=10, stride=1, padding=1)\n","        self.bn5 = nn.BatchNorm1d(64)   \n","        self.pool2 = nn.MaxPool1d(self.maxpool, padding= 1)\n","\n","        self.conv6 = nn.Conv1d(in_channels=64, out_channels=64, kernel_size=10, stride=1, padding=1)\n","        self.bn6 = nn.BatchNorm1d(64)\n","        self.conv7 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=10, stride=1, padding=1)\n","        self.bn7 = nn.BatchNorm1d(64)\n","        self.conv8 = nn.Conv1d(in_channels=128, out_channels=128, kernel_size=10, stride=1, padding=1)\n","        self.bn8 = nn.BatchNorm1d(128)\n","        \n","        self.conv_open = conv8_open\n","        self.pools = pool\n","        self.ave_pool = nn.AdaptiveAvgPool1d(self.pools)\n","        \n","        # FC\n","        self.fc1_num = fc1\n","        self.fc1 = nn.Linear(128*self.pools, self.fc1_num)\n","        self.fc2 = nn.Linear(self.fc1_num, 64)\n","        self.fc3 = nn.Linear(64, 3)\n","\n","        self.soft = nn.Softmax(dim=1)\n","\n","        self.drop = nn.Dropout(0.1)\n","\n","\n","    def forward(self, input1):\n","        output = F.celu(self.conv1(input1))\n","        output = F.celu(self.conv2(output))\n","        output = self.bn2(output)\n","        output = self.pool(output) \n","        \n","        output = F.celu(self.conv4(output))     \n","        output = F.celu(self.conv5(output)) \n","        output = self.bn5(output)\n","        output = self.pool(output)   \n","        output =  F.celu(self.conv6(output))\n","        output =  F.celu(self.conv7(output))\n","\n","        if self.conv_open:\n","            output =  F.elu(self.conv8(output))\n","        \n","        output = self.ave_pool(output)\n","        \n","        output = output.view(-1, 128*self.pools) \n","        \n","        con = self.fc1(output)\n","        con = self.drop(con)\n","        con = self.fc2(con)   \n","#         con = self.drop(con)\n","        con = self.fc3(con)\n","\n","        con = self.soft(con)\n","\n","        return con"]},{"cell_type":"markdown","metadata":{},"source":["# Start training\n"]},{"cell_type":"markdown","metadata":{},"source":["### Data split"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T07:47:04.428412Z","iopub.status.busy":"2023-11-23T07:47:04.428114Z","iopub.status.idle":"2023-11-23T07:47:04.444494Z","shell.execute_reply":"2023-11-23T07:47:04.443572Z","shell.execute_reply.started":"2023-11-23T07:47:04.428382Z"},"trusted":true},"outputs":[],"source":["skf = StratifiedKFold(n_splits=3, shuffle= True, random_state= 695)\n","folds = list(skf.split(all_df, all_df['label']))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T07:47:04.445888Z","iopub.status.busy":"2023-11-23T07:47:04.445621Z","iopub.status.idle":"2023-11-23T07:47:04.456039Z","shell.execute_reply":"2023-11-23T07:47:04.455326Z","shell.execute_reply.started":"2023-11-23T07:47:04.445858Z"},"trusted":true},"outputs":[],"source":["all_df['label'].value_counts()"]},{"cell_type":"markdown","metadata":{},"source":["### Define evaluetion and training function"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T07:47:04.457345Z","iopub.status.busy":"2023-11-23T07:47:04.457071Z","iopub.status.idle":"2023-11-23T07:47:04.465757Z","shell.execute_reply":"2023-11-23T07:47:04.464936Z","shell.execute_reply.started":"2023-11-23T07:47:04.457320Z"},"trusted":true},"outputs":[],"source":["def get_accuracy(dataloader):\n","    model.eval()\n","    running_loss, loss =0., 0.\n","    label_list, predicted_list = [], []\n","    total = 0\n","    correct = 0.\n","    with torch.no_grad():\n","        for i, (X, _, y) in enumerate(dataloader):\n","            images, labels = X.squeeze().float().to(device), y.squeeze().type(torch.LongTensor).to(device)\n","            # calculate outputs by running images through the network\n","            predictions = model(images)\n","            loss = criterion(predictions, labels)\n","            running_loss += loss.item()     \n","            loss = running_loss / (i+1)\n","\n","            # the class with the highest energy is what we choose as prediction\n","            predicted = predictions.to('cpu').argmax(1)\n","            label = np.array(labels.to('cpu'), dtype= int)\n","            label_list += list(label.ravel())\n","            predicted_list += list(predicted.ravel())\n","\n","        accuracy = accuracy_score(label_list, predicted_list)\n","    return label_list, predicted_list, loss, accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T07:47:04.467331Z","iopub.status.busy":"2023-11-23T07:47:04.467090Z","iopub.status.idle":"2023-11-23T07:47:04.481964Z","shell.execute_reply":"2023-11-23T07:47:04.481046Z","shell.execute_reply.started":"2023-11-23T07:47:04.467309Z"},"trusted":true},"outputs":[],"source":["def training(model, criterion, optimizer, name):\n","    for f, fold in enumerate(folds):\n","        higest_train_acc = 0\n","        before_acc = 0.\n","        higest_train = []\n","        higest_test =[]\n","\n","        # splite train & test\n","        train_df = all_df.loc[fold[0]]\n","        test_df = all_df.loc[fold[1]]\n","\n","        # make dataloader\n","        trans_comp = transforms.Compose([transforms.ToTensor()]) \n","        train_dataset = CustomImageDataset(train_df, transform=trans_comp)\n","        test_dataset = CustomImageDataset(test_df, transform=trans_comp)\n","        train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, drop_last=True)\n","        test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n","\n","        train_loss_list = []\n","        train_acc_list = []\n","        test_loss_list = []\n","        test_acc_list = []\n","\n","\n","        for epoch in range(150):\n","            running_loss = 0.0\n","\n","            for i, (X, _, y) in enumerate(train_dataloader, 0):\n","                model.train()\n","\n","                # get the inputs; data is a list of [inputs, labels]\n","                inputs, labels = X.squeeze().float().to(device), y.squeeze().type(torch.LongTensor).to(device)\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward + backward + optimize\n","                outputs = model(inputs)\n","                loss = criterion(outputs, labels)\n","                loss.backward()\n","                optimizer.step()\n","\n","                # print statistics\n","                running_loss += loss.item()\n","\n","            # training accuracy\n","            train_label_list, train_predicted_list, train_loss, train_acc = get_accuracy(train_dataloader)\n","\n","            # test accuracy\n","            test_label_list, test_predicted_list, test_loss, test_acc = get_accuracy(test_dataloader)\n","            \n","\n","            train_loss_list.append(train_loss)\n","            train_acc_list.append(train_acc)\n","            test_loss_list.append(test_loss)\n","            test_acc_list.append(test_acc)\n","\n","            rc_mean = recall_score(test_label_list, test_predicted_list, average=None)\n","\n","            if test_acc > before_acc and rc_mean[0] == 1:\n","                print(train_acc, test_acc)\n","                print(rc_mean)\n","                before_acc = test_acc\n","                higest_train_acc = train_acc\n","                print(\"{}.pth\".format(name+'_fold_'+str(f)+\"_epoch_\" +str(epoch)))\n","                torch.save(model.state_dict(), \"{}.pth\".format(name+'_fold_'+str(f)+\"_epoch_\" +str(epoch)))\n","\n","        higest_train.append(higest_train_acc)\n","        higest_test.append(before_acc)\n","        print('higest_train_acc', higest_train_acc, \"before_acc\", before_acc)\n","\n","    #     break\n","    print('Train_mean', np.mean(higest_train))\n","    print('Test_mean', np.mean(higest_test))\n","    print(\"Data Length\", len(higest_train))\n","    \n","    return train_loss_list, train_acc_list, test_loss_list, test_acc_list"]},{"cell_type":"markdown","metadata":{},"source":["### Start training\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T07:47:04.483445Z","iopub.status.busy":"2023-11-23T07:47:04.483093Z"},"trusted":true},"outputs":[],"source":["# compile network\n","model = Network(pool = 10, fc1=1024, maxpool=4, conv8_open=False).to(device)\n","weight = torch.tensor([1/5, 1/24, 1/29]).to(device)\n","criterion = nn.CrossEntropyLoss(weight=weight)\n","optimizer = optim.SGD(model.parameters(), lr=0.02, weight_decay= 0.0001) #lr * batch_size要一樣\n","name = 'control_adhd_ts_try'\n","\n","train_loss_list, train_acc_list, test_loss_list, test_acc_list = training(model, criterion, optimizer, name)"]},{"cell_type":"markdown","metadata":{},"source":["### train/test accuracies plot"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# record\n","df_result = pd.DataFrame()\n","df_result['train_loss'] = train_loss_list\n","df_result['train_acc'] = train_acc_list\n","df_result['test_loss'] = test_loss_list\n","df_result['test_acc'] = test_acc_list\n","df_result['epoch'] = [int(x) + 1 for x in df_result.index]\n","\n","# plot\n","plt.figure(figsize= (7, 3))\n","plt.plot(df_result['epoch'].iloc[:100], df_result['train_acc'].iloc[:100], label= 'train_acc')\n","plt.plot(df_result['epoch'].iloc[:100], df_result['test_acc'].iloc[:100], label= 'test_acc')\n","plt.legend(fontsize= 15 )\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.title('Accuracy plot')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Reproduction"]},{"cell_type":"markdown","metadata":{},"source":["### Load best model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.load_state_dict(torch.load('/kaggle/working/control_adhd_ts_try_fold_2_epoch_0.pth'))"]},{"cell_type":"markdown","metadata":{},"source":["### Reporduction & cofusion matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_df = all_df.loc[folds[-1][0]]\n","test_df = all_df.loc[folds[-1][1]]\n","trans_comp = transforms.Compose([transforms.ToTensor()]) \n","train_dataset = CustomImageDataset(train_df, transform=trans_comp)\n","test_dataset = CustomImageDataset(test_df, transform=trans_comp)\n","train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, drop_last=True)\n","test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n","train_label_list, train_predicted_list, train_loss, train_acc = get_accuracy(train_dataloader)\n","test_label_list, test_predicted_list, test_loss, test_acc = get_accuracy(test_dataloader)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["len(train_label_list), len(test_label_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["cm = confusion_matrix(train_label_list, train_predicted_list)\n","plt.figure(figsize = (4,3))\n","sns.heatmap(cm, annot = True, fmt='.20g', cmap= sns.color_palette(\"Blues\", as_cmap=True), xticklabels= ['Control', 'ADHD', 'TS'], yticklabels= ['Control', 'ADHD', 'TS'])\n","plt.title('Training CM')\n","plt.xlabel('Predicted')\n","plt.ylabel('Actual')\n","plt.show()\n","\n","cm = confusion_matrix(test_label_list, test_predicted_list)\n","plt.figure(figsize = (4, 3))\n","sns.heatmap(cm, annot = True, fmt='.20g', cmap= sns.color_palette(\"Blues\", as_cmap=True), xticklabels= ['Control', 'ADHD', 'TS'], yticklabels= ['Control', 'ADHD', 'TS'])\n","plt.title('Test CM')\n","plt.xlabel('Predicted')\n","plt.ylabel('Actual')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Explainable AI -- Intergradient Greadient"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def minmax(x):\n","    return (x - x.min())/(x.max() - x.min())"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.eval()\n","ig = IntegratedGradients(model)\n","\n","for batch, (X, _, y) in enumerate(train_dataloader):\n","    input1 = X.squeeze().float().to(device)\n","    attributions, delta = ig.attribute(input1, target=0, return_convergence_delta=True)\n","    \n","    df_cam = pd.DataFrame()\n","    for nums, att in enumerate(attributions):\n","        df_cam[f'{nums}_cam0'] = att.cpu()[0]\n","        df_cam[f'{nums}_cam1'] = att.cpu()[1]\n","        df_cam[f'{nums}_cam2'] = att.cpu()[2]\n","        df_cam[f'{nums}_cam3'] = att.cpu()[3]\n","    df_rolling = minmax(df_cam.rolling(50).mean().bfill())\n","    \n","    for nums in range(len(attributions)):\n","        # First plot\n","        yy = np.linspace(-0.01, 1, 100)\n","        cmapp1 = [(1, 0, 0, a*0.5) for a in df_rolling[f'{nums}_cam0']]\n","        colors = cmapp1\n","        # 繪製水平色彩漸進圖\n","        fig, ax = plt.subplots(figsize=(40, 15))\n","\n","        for i in range(len(colors)):\n","            ax.plot(np.ones_like(yy) * i, yy, color=colors[i], linewidth=4)\n","            \n","        plt.scatter(np.arange(0, len(df_rolling[f'{nums}_cam0']), 1), df_rolling[f'{nums}_cam0'], label= 'region1', c='black', linewidth=10)\n","        plt.show()"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":3411696,"sourceId":5946021,"sourceType":"datasetVersion"}],"dockerImageVersionId":30528,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
